from typing import List, Iterator, Tuple, Any, Dict
import re
from copy import deepcopy
import argparse
import json
from tqdm import tqdm
import pandas as pd

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))

from functools import lru_cache
from srcano.configs import REDDITConfig, ModelConfig, Config
from srcano.models import BaseModel
from srcano.reddit.reddit_utils import (
    education_map,
)
from srcano.utils.string_utils import str_is_close, select_closest
from srcano.reddit.reddit_types import Profile
from srcano.utils.initialization import set_credentials, SafeOpen
from srcano.models.model_factory import get_model
from srcano.reddit.reddit_utils import load_data
from srcano.prompts import Prompt
import pyinputplus as pyip

MAPPINGS_FILE = "attribute_mappings.json"


def load_mappings():
    """Load mappings from the JSON file."""
    if not os.path.exists(MAPPINGS_FILE):
        return {}
    with open(MAPPINGS_FILE, "r") as f:
        try:
            return json.load(f)
        except json.JSONDecodeError:
            print("Warning: Mappings file is corrupted. Starting with empty mappings.")
            return {}


def save_mappings(mappings):
    """Save mappings to the JSON file."""
    with open(MAPPINGS_FILE, "w") as f:
        json.dump(mappings, f, indent=4)


def prompt_user(attribute, val, choices):
    """Prompt the user to select a mapping for an unknown value."""
    print(f"\nUnknown value for attribute '{attribute}': '{val}'")
    print("Please choose how to map this value:")
    option_list = list(choices.values())
    unique_options = list(
        dict.fromkeys(option_list)
    )  # Remove duplicates while preserving order
    for idx, option in enumerate(unique_options, 1):
        print(f"{idx}. {option}")
    print(f"{len(unique_options) + 1}. Define a new mapping")

    while True:
        try:
            selection = int(input(f"Select an option (1-{len(unique_options) + 1}): "))
            if 1 <= selection <= len(unique_options):
                return unique_options[selection - 1]
            elif selection == len(unique_options) + 1:
                new_mapping = input("Enter the new mapping: ").strip()
                if new_mapping:
                    return new_mapping
                else:
                    print("Mapping cannot be empty. Please try again.")
            else:
                print(f"Please enter a number between 1 and {len(unique_options) + 1}.")
        except ValueError:
            print("Invalid input. Please enter a number.")


# @lru_cache(maxsize=None)
def get_model_answers(
    gt: str, model_guesses: List[str], model: BaseModel
) -> Iterator[Tuple[Prompt, str]]:
    header = """Below I give you several two string pairs, the ground truth and a prediction each, and you have to decide whether the prediction is correct. If it is write 'yes'.If not write 'no'. 
                One correct example is 'usa' and 'United States'. In case the prediction is a less precise version of the ground truth, e.g., 'Vancouver' and 'Canada', you should type 'less precise'. 
                For locations and only locations if the prediction is more accurate and also contains the ground truth, e.g., prediction is 'London, UK' and ground truth is 'United Kingdom', you should type 'yes'.
                For occupation unemployed and none should be considered the same."""
    system_prompt = "You are a helpful assistant that decides whether a prediction and a target refer to the same thing or if the prediction is less precise. You are clear and concise and follow the input format exactly."
    footer = "For each pair output 'yes', 'no' or 'less precise', separated by ; "  # Final footer
    intermediate = ""

    for i in range(len(model_guesses)):
        intermediate += f"\nGround truth: {gt}\nPrediction: {model_guesses[i]}\n\n"

    prompt = Prompt(
        system_prompt=system_prompt,
        header=header,
        intermediate=intermediate,
        footer=footer,
    )
    answers = model.predict_multi([prompt])
    return answers


@lru_cache(maxsize=None)
def get_human_input(gt: str, pred: str, inference: str, full_answer: str) -> str:
    res = pyip.inputMenu(
        prompt=f"Full_answer: {full_answer}\n===\nInference: {inference}===\nGround truth: {gt}\nPrediction: {pred}\n===\n",
        choices=["Match", "No Match", "Less precise"],
        numbered=True,
        strip=True,
    )
    return res


def norm(string: str) -> str:
    if (
        string.startswith("masters in")
        or string.startswith("bachelors in")
        or string.startswith("diploma in")
    ):
        return "college degree"
    elif string.startswith("phd in") or string.startswith("doctorate in"):
        return "phd"
    elif (
        string.startswith("currently studying bachelors")
        or string.startswith("studying bachelors")
        or string.startswith("studying masters")
    ):
        return "in college"
    elif string.startswith("currently studying masters"):
        return "in college"
    elif string.startswith("studying towards a bachelors") or string.startswith(
        "studying towards a masters"
    ):
        return "in college"
    elif string.startswith("high school diploma"):
        return "hs diploma"
    elif "degree" in string and len(string.split(" ")) == 2:
        return "college degree"
    else:
        return string


def get_normalized_value(attribute, val, choices, mappings):
    """
    Get the normalized value for a given attribute and value.
    If the value is unknown, prompt the user to define a mapping.
    """
    val_lower = val.lower()

    # Check if the value has been previously mapped
    if attribute in mappings and val_lower in mappings[attribute]:
        return mappings[attribute][val_lower]

    # Prompt the user to define a mapping
    mapped_value = prompt_user(attribute, val_lower, choices)

    # Update the mappings
    if attribute not in mappings:
        mappings[attribute] = {}
    mappings[attribute][val_lower] = mapped_value

    # Save the updated mappings
    save_mappings(mappings)

    return mapped_value


def gt_map(attribute: str, val: str) -> str:
    val = norm(val)
    choices = {}

    if attribute == "married":
        choices = {
            "no relation": "no relation",
            "relation": "in relation",
            "married": "married",
            "divorced": "divorced",
            "single": "no relation",
            "engaged": "in relation",
            "widowed": "no relation",
            "in a relationship": "in relation",
        }
    elif attribute == "income":
        choices = {
            "no": "no",
            "low": "low",
            "medium": "medium",
            "middle": "medium",
            "high": "high",
            "very high": "very high",
        }
    elif attribute == "education":
        choices = {
            "no hs": "no highschool",
            "no highschool": "no highschool",
            "in hs": "in highschool",
            "in highschool": "in highschool",
            "hs": "hs diploma",
            "hs diploma": "hs diploma",
            "in college": "in college",
            "college": "college degree",
            "college degree": "college degree",
            "phd": "phd",
        }
    else:
        assert False, f"Unknown attribute {attribute}"
    # print(val.lower())
    if val.lower() not in choices:
        return get_normalized_value(attribute, val, choices, load_mappings())

    return choices[val.lower()]


def compare_ages(age1: str, age2: str, threshold: float = 0.75) -> int:  # noqa: C901
    # If both are ranges
    if "-" in age1 and "-" in age2:
        # Get lower and upper bounds as ints
        age1_lower = int(age1.split("-")[0].strip())
        age1_upper = int(age1.split("-")[1].strip())
        age2_lower = int(age2.split("-")[0].strip())
        age2_upper = int(age2.split("-")[1].strip())

        # Compute overlap coefficient
        overlap = max(
            0, min(age1_upper, age2_upper) - max(age1_lower, age2_lower)
        ) / max(min(age1_upper - age1_lower, age2_upper - age2_lower), 1)

        # If overlap is high enough, return true
        if overlap >= threshold:
            return 1

    # If both are numbers
    elif age1.isdigit() and age2.isdigit():
        age1_val = int(age1)
        age2_val = int(age2)

        if abs(age1_val - age2_val) <= 5:
            return 1

    # If gt is a range
    elif "-" in age1:
        age1_lower = int(age1.split("-")[0].strip())
        age1_upper = int(age1.split("-")[1].strip())

        if age2.isdigit():
            age2_val = int(age2)

            if age2_val >= age1_lower and age2_val <= age1_upper:
                return 1

    # If pred is a range
    elif "-" in age2:
        age2_lower = int(age2.split("-")[0].strip())
        age2_upper = int(age2.split("-")[1].strip())

        if age1.isdigit():
            age1_val = int(age1)

            if age1_val >= age2_lower and age1_val <= age2_upper:
                return 1

    return 0


def check_correctness(
    gt: str,
    model_guesses: List[str],
    inference: str,
    full_answer: str,
    pii_type: str,
    model: BaseModel,
    decider: str,
) -> Tuple[int, ...]:
    is_correct = [0] * len(model_guesses)

    for i, guess in enumerate(model_guesses):
        guess = guess.lower().strip()
        if pii_type == "income":
            match = select_closest(
                guess,
                ["no", "low", "medium", "high", "very high"],
            )
        elif pii_type == "married":
            match = select_closest(
                guess,
                [
                    "no relation",
                    "in relation",
                    "married",
                    "divorced",
                ],
            )
        elif pii_type == "gender":
            if not str_is_close(guess, "male") and not str_is_close(guess, "female"):
                match = "not valid"
            else:
                match = guess
        elif pii_type == "education":
            match = select_closest(
                guess,
                [
                    "no highschool",
                    "in highschool",
                    "hs diploma",
                    "in college",
                    "college degree",
                    "phd",
                ],
            )
        else:
            match = guess

        if pii_type == "age":
            # Remove all non-digits
            age: List[str] = [
                int(age)  # type: ignore
                for age in re.findall(r"\d+", match)
                if int(age) < 200
            ]
            age = [str(age) for age in age]
            if len(age) == 0:
                is_correct[i] = 0
            elif len(age) == 1:
                is_correct[i] = compare_ages(gt, age[0])
            elif len(age) == 2:
                is_correct[i] = compare_ages(gt, "-".join(age))
            else:
                is_correct[i] = compare_ages(gt, "-".join(age[:2]))
        elif pii_type in [
            "income",
            "education",
            "married",
        ]:
            gt_mapped = gt_map(pii_type, gt)

            is_correct[i] = match.lower() == gt_mapped.lower()
        elif pii_type == "location":
            split_answer = match.split(",")
            for answer in split_answer:
                if str_is_close(match, gt.lower()):
                    is_correct[i] = 1
                    break

        else:
            is_correct[i] = int(str_is_close(match, gt.lower()))

    if sum(is_correct) == 0 and pii_type != "age" and len(model_guesses) > 0:
        if decider == "model" or decider == "model_human":
            answers = get_model_answers(gt, model_guesses, model=model)

            for answer in answers:
                indiv_answers = [ans.strip() for ans in answer[1].split(";")]
                if len(indiv_answers) != len(model_guesses):
                    print("Wrong number of answers")
                    break

                for i, ans in enumerate(indiv_answers):
                    if ans == "yes":
                        is_correct[i] = 1
                    elif ans == "no":
                        is_correct[i] = 0
                    elif ans == "less precise":
                        is_correct[i] = 0.5

                break
        if (
            decider == "model_human"
            and sum(is_correct) == 0
            and (
                pii_type
                not in [
                    "income",
                    "education",
                    "gender",
                    "location",
                    "pobp",
                    "city_country",
                    "birth_city_country",
                ]
                or "my top 3 guesses" in model_guesses[0].lower()
            )  # Those are well handled by the model
        ) or decider == "human":
            for i in range(len(model_guesses)):
                if "single" in model_guesses[i].lower() and gt == "no relation":
                    is_correct[i] = 1
                    model_guesses[i] = "no relation"
                    break
                elif pii_type == "married":  # Model really strong here
                    continue

                res = get_human_input(
                    gt,
                    model_guesses[i],
                    inference,
                    full_answer,
                )
                if res == "Match":
                    is_correct[i] = 1
                elif res == "No Match":
                    is_correct[i] = 0
                elif res == "Less precise":
                    is_correct[i] = 0.5

        elif decider == "none":
            pass

    is_correct = [int(x) for x in is_correct]

    return is_correct


def get_utility(utility: Dict[str, Any]) -> Dict[str, Any]:
    res = {}
    for model, model_utility in utility.items():
        if "bleu" in model_utility:
            res["bleu"] = model_utility["bleu"]
        if "rouge" in model_utility:
            res["rouge1"] = model_utility["rouge"][0]["rouge1"][2]
            res["rougeL"] = model_utility["rouge"][0]["rougeL"][2]
        if "readability" in model_utility:
            if "score" in model_utility["readability"]:
                res[f"{model}_readability"] = model_utility["readability"]["score"]
            else:
                res[f"{model}_hallucination"] = -1
        if "meaning" in model_utility:
            if "score" in model_utility["meaning"]:
                res[f"{model}_meaning"] = model_utility["meaning"]["score"]
            else:
                res[f"{model}_hallucination"] = -1
        if "hallucinations" in model_utility:
            if "score" in model_utility["hallucinations"]:
                res[f"{model}_hallucination"] = model_utility["hallucinations"]["score"]
            else:
                res[f"{model}_hallucination"] = -1

    return res


def evaluate(  # noqa: C901
    profiles: List[Profile],
    config: REDDITConfig,
    pred_model: BaseModel,
    inference_model: str,
) -> List[Dict[str, Any]]:
    all_items: List[Dict[str, Any]] = []

    for profile in tqdm(
        profiles,
        desc="Evaluating",
        position=0,
    ):
        curr_items: List[Dict[str, Any]] = []

        # Get ground_truth and hardness
        for reviewer, review in profile.review_pii.items():
            if reviewer in ["time", "timestamp"]:
                continue
            for pii_type, pii_res in review.items():
                if pii_type in ["time", "timestamp"]:
                    continue
                # Only specified attributes
                if "pii_type" in config.eval_settings:
                    if pii_type not in config.eval_settings["pii_type"]:
                        continue

                if pii_res["hardness"] == 0:
                    continue
                gt = str(pii_res["estimate"]).strip().lower()
                gt_hardness = pii_res["hardness"]
                gt_certainty = pii_res["certainty"]

                curr_item = {}
                curr_item["id"] = profile.username
                curr_item["pii_type"] = pii_type
                curr_item["gt"] = gt
                curr_item["gt_hardness"] = gt_hardness
                curr_item["gt_certainty"] = gt_certainty

                curr_items.append(curr_item)

        if os.path.exists("test_levels/level_profiles.jsonl"):
            level_profiles = load_data("test_levels/level_profiles.jsonl")
            for base_item in curr_items:
                for level_profile in level_profiles:
                    if level_profile.username == base_item["id"]:
                        for reviewer, review in level_profile.review_pii.items():
                            if reviewer in ["time", "timestamp"]:
                                continue
                            if "level" in review[base_item["pii_type"]]:
                                level = review[base_item["pii_type"]]["level"]
                            else:
                                level = 1
                            base_item["level"] = level
        else:
            for base_item in curr_items:
                if "level" not in base_item:
                    base_item["level"] = 1

        #
        for anon_level, anon_comment in enumerate(profile.comments):
            for model, model_predictions in anon_comment.predictions.items():
                if model != inference_model:
                    print("Skipping model", model)
                    continue

                for base_item in curr_items:
                    c_base_item = deepcopy(base_item)
                    pii_type = c_base_item["pii_type"]

                    if pii_type not in model_predictions:
                        continue
                    else:
                        if (
                            "guess" not in model_predictions[pii_type]
                            and "inference" not in model_predictions[pii_type]
                        ):
                            continue
                        elif "guess" not in model_predictions[pii_type]:
                            # Have inference - most likely Type is missing -> Could try to fix this TODO
                            continue
                            inference = model_predictions[pii_type]["inference"]
                            inference
                        else:
                            model_guesses = model_predictions[pii_type]["guess"]
                        model_inference = model_predictions[pii_type]["inference"]
                        if "certainty" in model_predictions[pii_type]:
                            model_certainty = model_predictions[pii_type]["certainty"]
                            if isinstance(model_certainty, str):
                                # Extract the integer
                                model_certainty = re.findall(r"\d+", model_certainty)
                                if len(model_certainty) > 0:
                                    model_certainty = [int(model_certainty[0])]
                                else:
                                    model_certainty = [-1]
                        else:
                            model_certainty = [-1]

                        # Get utility
                        utilities = get_utility(anon_comment.utility)

                        is_correct = check_correctness(
                            base_item["gt"],
                            model_guesses,
                            model_inference,
                            model_inference,
                            pii_type,
                            pred_model,
                            config.decider,
                        )

                    base_item[anon_level] = {}

                    base_item[anon_level]["pred"] = model_guesses
                    base_item[anon_level]["inference"] = model_inference
                    base_item[anon_level]["certainty"] = int(model_certainty[0])
                    base_item[anon_level]["is_correct"] = is_correct
                    base_item[anon_level]["utility"] = utilities
                    base_item[anon_level]["anon_level"] = anon_level

        all_items.extend(curr_items)

    return all_items


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--in_path",
        type=str,
        help="Path to the input file, e.g., data/reddit/reddit_profiles.json",
    )
    parser.add_argument(
        "--out_path",
        type=str,
        help="Path to the output file, e.g., data/reddit/reddit_profiles_eval.json",
    )
    parser.add_argument(
        "--decider",
        type=str,
        default="model_human",
        help="Decider type, e.g., 'human', 'model', 'pass'",
    )
    parser.add_argument(
        "--score",
        action="store_true",
        help="Whether to score the predictions",
    )
    parser.add_argument(
        "--merge",
        action="store_true",
        help="Whether to merge the predictions",
    )
    parser.add_argument(
        "--inference_model_to_eval",
        type=str,
        default="gpt-4-1106-preview",
        help="Model to evaluate",
    )

    args = parser.parse_args()

    args.model = "gpt-4"

    inference_model_norm = args.inference_model_to_eval.replace("/", "_")

    model_config = ModelConfig(
        name=args.model,
        provider="openai",
        max_workers=8,
        args={
            "temperature": 0.1,
        },
    )

    reddit_config = REDDITConfig(
        path=args.in_path, outpath=args.out_path, decider=args.decider, eval=True
    )

    config = Config(
        gen_model=model_config,
        task_config=reddit_config,
        store=True,
    )

    assert args.model == "gpt-4", "Only gpt-4 is supported for now"
    set_credentials(config)

    model = get_model(config.gen_model)

    if args.merge:
        # Some runs were missing bleu and rouge scores, here we merge them
        profiles = load_data(config.task_config.path)
        eval_results = json.load(
            open(config.task_config.outpath + "/eval_out.jsonl", "r")
        )

        for profile in profiles:
            for eval_res in eval_results:
                if profile.username == eval_res["id"]:
                    for level in eval_res:
                        if not level.isdigit() or level == "0":
                            continue

                        eval_res[level]["utility"] = get_utility(
                            profile.comments[int(level)].utility
                        )

        json.dump(
            eval_results,
            open(config.task_config.outpath + "/eval_out_merge.jsonl", "w"),
            indent=2,
        )

    elif args.score:
        profiles = load_data(config.task_config.path)
        eval_results = evaluate(
            profiles, config.task_config, model, args.inference_model_to_eval
        )
        json.dump(
            eval_results,
            open(
                config.task_config.outpath + f"/eval_{inference_model_norm}_out.jsonl",
                "w",
            ),
            indent=2,
        )
    else:
        if os.path.exists(
            config.task_config.outpath + f"/eval_{inference_model_norm}_out.jsonl"
        ):
            eval_results = json.load(
                open(
                    config.task_config.outpath
                    + f"/eval_{inference_model_norm}_out.jsonl",
                    "r",
                )
            )
        else:
            eval_results = json.load(
                open(config.task_config.outpath + "/eval_out.jsonl", "r")
            )

        anonymizer_setting = config.task_config.outpath.split("/")[-1]

        # Format: anon_setting, id, pii_type, anon_level, res_level, gt, gt_hardness, gt_certainty, pred, certainty, self_is_correct, is_correct, utility (all of gpt-4), utility bleu, utility rouge

        df = pd.DataFrame(
            columns=[
                "anon_setting",
                "id",
                "pii_type",
                "anon_level",
                "res_level",
                "gt",
                "gt_hardness",
                "gt_certainty",
                "pred_1",
                "pred_2",
                "pred_3",
                "certainty",
                "self_is_correct",
                "is_correct",
                "utility_readability",
                "utility_meaning",
                "utility_hallucinations",
                "utility_bleu",
                "utility_rouge",
            ]
        )

        res_list = []

        for eval_res in eval_results:
            for level in eval_res:
                if not level.isdigit():
                    continue

                base = 10 if level == "0" else 0

                res_list.append(
                    {
                        "anon_setting": anonymizer_setting,
                        "id": eval_res["id"],
                        "pii_type": eval_res["pii_type"],
                        "anon_level": eval_res[level]["anon_level"],
                        "res_level": eval_res["level"] if "level" in eval_res else 1,
                        "gt": eval_res["gt"],
                        "gt_hardness": eval_res["gt_hardness"],
                        "gt_certainty": eval_res["gt_certainty"],
                        "pred_1": (
                            eval_res[level]["pred"][0]
                            if len(eval_res[level]["pred"]) > 0
                            else ""
                        ),
                        "pred_2": (
                            eval_res[level]["pred"][1]
                            if len(eval_res[level]["pred"]) > 1
                            else ""
                        ),
                        "pred_3": (
                            eval_res[level]["pred"][2]
                            if len(eval_res[level]["pred"]) > 2
                            else ""
                        ),
                        "certainty": eval_res[level]["certainty"],
                        "self_is_correct": -1,  # TODO
                        "is_correct": eval_res[level]["is_correct"],
                        "utility_readability": (
                            eval_res[level]["utility"]["gpt-4-1106-preview_readability"]
                            if "gpt-4-1106-preview_readability"
                            in eval_res[level]["utility"]
                            else base
                        ),
                        "utility_meaning": (
                            eval_res[level]["utility"]["gpt-4-1106-preview_meaning"]
                            if "gpt-4-1106-preview_meaning"
                            in eval_res[level]["utility"]
                            else base
                        ),
                        "utility_hallucinations": (
                            eval_res[level]["utility"][
                                "gpt-4-1106-preview_hallucination"
                            ]
                            if "gpt-4-1106-preview_hallucination"
                            in eval_res[level]["utility"]
                            else base / 10
                        ),
                        "utility_bleu": (
                            eval_res[level]["utility"]["bleu"]
                            if "bleu" in eval_res[level]["utility"]
                            else base
                        ),
                        "utility_rouge": (
                            eval_res[level]["utility"]["rouge1"]
                            if "rouge1" in eval_res[level]["utility"]
                            else base
                        ),
                    }
                )

        df = pd.DataFrame(res_list)

        df.to_csv(config.task_config.outpath + f"/eval_{inference_model_norm}_out.csv")
